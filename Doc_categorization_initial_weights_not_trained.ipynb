{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages (4.37.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in c:\\users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->transformers) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "uploaded_file_path = \"merged_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(uploaded_file_path)\n",
    "\n",
    "input_data = df[\"Text\"].tolist()\n",
    "labels = df[\"Label\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = [str(sentence) for sentence in input_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, val_texts, train_labels, val_labels = train_test_split(input_data, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokenized = tokenizer(train_texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "train_input_ids = train_tokenized[\"input_ids\"]\n",
    "train_attention_mask = train_tokenized[\"attention_mask\"]\n",
    "train_labels = torch.tensor(train_labels, dtype=torch.long)\n",
    "train_dataset = TensorDataset(train_input_ids, train_attention_mask, train_labels)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_tokenized = tokenizer(val_texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "val_input_ids = val_tokenized[\"input_ids\"]\n",
    "val_attention_mask = val_tokenized[\"attention_mask\"]\n",
    "val_labels = torch.tensor(val_labels, dtype=torch.long)\n",
    "val_dataset = TensorDataset(val_input_ids, val_attention_mask, val_labels)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total entries in the dataset: 58619\n"
     ]
    }
   ],
   "source": [
    "total_entries = df.shape[0]\n",
    "print(f'Total entries in the dataset: {total_entries}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=9, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', config=config)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "total_epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "label_counts = df[\"Label\"].value_counts()\n",
    "label_counts.plot(kind='bar', xlabel='Labels', ylabel='Count', title='Label Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:  22%|██▏       | 5230/23448 [10:20:28<36:01:21,  7.12s/it, Loss=0.00109] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m input_ids_batch, attention_mask_batch, labels_batch \u001b[38;5;241m=\u001b[39m [t\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m batch]\n\u001b[0;32m      7\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m----> 8\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m     10\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:1198\u001b[0m, in \u001b[0;36mRobertaForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1193\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1194\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1198\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1199\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1203\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1204\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1205\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1206\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1207\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1208\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1209\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1210\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(sequence_output)\n",
      "File \u001b[1;32mc:\\Users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:835\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    826\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m    828\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[0;32m    829\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m    830\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    833\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[0;32m    834\u001b[0m )\n\u001b[1;32m--> 835\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    836\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    837\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    838\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    839\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    840\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    841\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    843\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    844\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    845\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    846\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    847\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    848\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:524\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    513\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    514\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    515\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    521\u001b[0m         output_attentions,\n\u001b[0;32m    522\u001b[0m     )\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 524\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    531\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    534\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:455\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    452\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    453\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[1;32m--> 455\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    456\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    458\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[0;32m    460\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages\\transformers\\pytorch_utils.py:236\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[1;32m--> 236\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:467\u001b[0m, in \u001b[0;36mRobertaLayer.feed_forward_chunk\u001b[1;34m(self, attention_output)\u001b[0m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[1;32m--> 467\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    468\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[0;32m    469\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[1;32mc:\\Users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:365\u001b[0m, in \u001b[0;36mRobertaIntermediate.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    364\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 365\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    366\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate_act_fn(hidden_states)\n\u001b[0;32m    367\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[1;32mc:\\Users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(total_epochs):\n",
    "    epoch_progress = tqdm(train_dataloader, desc=f'Epoch {epoch + 1}/{total_epochs}', dynamic_ncols=True)\n",
    "    \n",
    "    for batch in epoch_progress:\n",
    "        input_ids_batch, attention_mask_batch, labels_batch = [t.to(device) for t in batch]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids_batch, attention_mask=attention_mask_batch, labels=labels_batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_progress.set_postfix({'Loss': loss.item()}, refresh=True)\n",
    "    \n",
    "    epoch_progress.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"C:/Users/kkysh/Downloads/ML KA MAAL/Doc Categorization Project/models/your_model_directory\")\n",
    "tokenizer.save_pretrained(\"C:/Users/kkysh/Downloads/ML KA MAAL/Doc Categorization Project/models/your_model_directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "for batch in val_dataloader:\n",
    "    input_ids_batch, attention_mask_batch, labels_batch = [t.to(device) for t in batch]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids_batch, attention_mask=attention_mask_batch)\n",
    "    \n",
    "    predictions = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
    "    \n",
    "    all_predictions.extend(predictions)\n",
    "    all_labels.extend(labels_batch.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "print(f'Validation Accuracy: {accuracy * 100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
