{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages (4.37.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in c:\\users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->transformers) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer\n",
    "from torch.utils.data import DataLoader, TensorDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload your CSV file to Colab and change the filename\n",
    "uploaded_file_path = \"merged_data_subset.csv\"\n",
    "\n",
    "# Read CSV using Pandas\n",
    "df = pd.read_csv(uploaded_file_path)\n",
    "\n",
    "# Extract input data and labels from your DataFrame\n",
    "input_data = df[\"Text\"].tolist()  # Change \"text_column\" to the actual column name in your CSV\n",
    "labels = df[\"Label\"].tolist()  # Change \"label_column\" to the actual column name in your CSV\n",
    "\n",
    "input_data = [str(sentence) for sentence in input_data]\n",
    "\n",
    "# Load pre-trained RoBERTa model and tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\kkysh\\AppData\\Local\\Temp\\ipykernel_4872\\2856087204.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels, dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "config = RobertaConfig.from_pretrained('roberta-base', num_labels=9)\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', config=config)\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "# Tokenize input data and convert labels to tensors\n",
    "tokenized_input = tokenizer(input_data, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "input_ids = tokenized_input[\"input_ids\"]\n",
    "attention_mask = tokenized_input[\"attention_mask\"]\n",
    "labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "# Create DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(input_ids, attention_mask, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# Device setup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Training loop (same as before)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 2500/2500 [4:49:21<00:00,  6.94s/it, Loss=0.0176]    \n"
     ]
    }
   ],
   "source": [
    "for epoch in range(total_epochs):\n",
    "    # Create a progress bar for the epoch\n",
    "    epoch_progress = tqdm(dataloader, desc=f'Epoch {epoch + 1}/{total_epochs}', dynamic_ncols=True)\n",
    "\n",
    "    for batch in epoch_progress:\n",
    "        input_ids_batch, attention_mask_batch, labels_batch = [t.to(device) for t in batch]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids_batch, attention_mask=attention_mask_batch, labels=labels_batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the progress bar with the current loss\n",
    "        epoch_progress.set_postfix({'Loss': loss.item()}, refresh=True)\n",
    "\n",
    "    # Close the progress bar for the epoch\n",
    "    epoch_progress.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=9, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions = []\n",
    "all_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'validation_dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m \u001b[43mvalidation_dataloader\u001b[49m:\n\u001b[0;32m      2\u001b[0m     input_ids_batch, attention_mask_batch, labels_batch \u001b[38;5;241m=\u001b[39m [t\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m batch]\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# Forward pass (no gradient calculation needed during evaluation)\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'validation_dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "for batch in validation_dataloader:\n",
    "    input_ids_batch, attention_mask_batch, labels_batch = [t.to(device) for t in batch]\n",
    "\n",
    "    # Forward pass (no gradient calculation needed during evaluation)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids_batch, attention_mask=attention_mask_batch)\n",
    "    \n",
    "    # Get predicted labels\n",
    "    predictions = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
    "\n",
    "    # Collect predictions and ground truth labels\n",
    "    all_predictions.extend(predictions)\n",
    "    all_labels.extend(labels_batch.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Labels: [6]\n"
     ]
    }
   ],
   "source": [
    "input_text = \"\"\"LPL - LPL-ROHINI (NATIONAL REFERENCE\n",
    "LAB)\n",
    "SECTOR - 18, BLOCK -E ROHINI\n",
    "DELHI 110085\n",
    "Name\n",
    "A/c Status\n",
    "Lab No.\n",
    "Ref By :\n",
    "Gender: Age:\n",
    "Report Status\n",
    "Reported\n",
    "Received\n",
    "Collected\n",
    "P\n",
    ":\n",
    ":\n",
    ":\n",
    ":\n",
    ":\n",
    ":\n",
    ": Final\n",
    "30 Years\n",
    "15/9/2017 3:37:00PM\n",
    "15/9/2017 3:59:23PM\n",
    "135091274 Male\n",
    "Dr. UNKNWON\n",
    "Mr. Z836\n",
    "19/9/2017 7:26:39PM\n",
    "Test Name Results Units Bio. Ref. Interval\n",
    "NAFLD FIBROSIS SCORE\n",
    "(Spectrophotometry)\n",
    "AST (SGOT) 11.00 U/L <50.00\n",
    "ALT (SGPT) 12.00 U/L <50.00\n",
    "Albumin 4.00 g/dL 3.50 - 5.20\n",
    "Glucose, Fasting 88.00 mg/dL 70.00 - 100.00\n",
    "Glucose, PP 101.00 mg/dL 70.00 - 140.00\n",
    "Platelet count 264.00 thou/mm3 150.00 - 450.00\n",
    "Height 165.00 cm\n",
    "Weight 56.00 kg\n",
    "NAFLD score 2.49 <-1.455\n",
    "Note\n",
    "1. Enhanced liver fibrosis test may be used to further characterize patients with indeterminate score.\n",
    "2. The test conducted in serum, plasma & whole blood.\n",
    "Interpretation\n",
    " -----------------------------------------------------------------------------\n",
    "| NAFLD FIBROSIS SCORE | REMARKS |\n",
    "|----------------------|------------------------------------------------------|\n",
    "| < -1.455 | Predictor of absence of significant fibrosis (F0-F2) |\n",
    "|----------------------|------------------------------------------------------|\n",
    "| -1.455- 0.675 | Indeterminate |\n",
    "|----------------------|------------------------------------------------------|\n",
    "| >0.675 | Predictor of presence of significant fibrosis (F3-F4)|\n",
    " -----------------------------------------------------------------------------\n",
    "Comment\n",
    "NAFLD fibrosis score is the most studied score which has been extensively validated in a large population of\n",
    "patients with Non Alcoholic Fatty Liver disease (NAFLD) for detecting significant fibrosis.. NAFLD is the most\n",
    "common cause of abnormal liver function tests in primary care. It is associated with obesity, insulin\n",
    "resistance, Diabetes, Dyslipidemia, hypertension and is considered as hepatic manifestation of the Metabolic\n",
    "syndrome. The pivotal issue in managing patients with NAFLD is the diagnosis of steatohepatitis & fibrosis at\n",
    "early stage. Liver biopsy (the Gold standard for diagnosing & assessing fibrosis) is not considered appropriate\n",
    "as first line tool for diagnosing fibrosis in unselected NAFLD patients. This test has high negative predictive\n",
    "value, thus can be used as first line test to rule out patients without advanced fibrosis thereby preventing\n",
    "unnecessary secondary care referrals.\n",
    "Uses\n",
    "· Metabolic syndrome / Diabetes with suspected NAFLD\n",
    "PatientReportSCSuperPanel.GENERAL_PANEL_ANALYTE_SC (Version: 6) *135091274*\n",
    "Page 1 of 2\n",
    "LPL - LPL-ROHINI (NATIONAL REFERENCE\n",
    "LAB)\n",
    "SECTOR - 18, BLOCK -E ROHINI\n",
    "DELHI 110085\n",
    "Name\n",
    "A/c Status\n",
    "Lab No.\n",
    "Ref By :\n",
    "Gender: Age:\n",
    "Report Status\n",
    "Reported\n",
    "Received\n",
    "Collected\n",
    "P\n",
    ":\n",
    ":\n",
    ":\n",
    ":\n",
    ":\n",
    ":\n",
    ": Final\n",
    "30 Years\n",
    "15/9/2017 3:37:00PM\n",
    "15/9/2017 3:59:23PM\n",
    "135091274 Male\n",
    "Dr. UNKNWON\n",
    "Mr. Z836\n",
    "19/9/2017 7:26:39PM\n",
    "Test Name Results Units Bio. Ref. Interval\n",
    "· Fatty liver index >60\n",
    "· Evidence of hepatic steatosis in USG (other causes of fatty liver disease such as alcoholic liver\n",
    "disease, viral hepatitis, drugs etc. excluded)\"\"\"\n",
    "\n",
    "tokenized_input = tokenizer(input_text, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "\n",
    "input_ids = tokenized_input[\"input_ids\"].to(device)\n",
    "attention_mask = tokenized_input[\"attention_mask\"].to(device)\n",
    "\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "logits = outputs.logits\n",
    "predicted_labels = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "\n",
    "print(\"Predicted Labels:\", predicted_labels)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
