{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages (4.37.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in c:\\users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: torch in c:\\users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages (2.2.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import transformers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'merged_data.csv'\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kkysh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kkysh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Text'] = df['Text'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token.isalnum() and token not in stop_words]\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Text'] = df['Text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: usual would tear around live room play toy one look minion sent practic cataton megan plan got dress earlier seen movi almost mistak consid littl young pg cartoon older cousin along brother mason often expos thing older like think surround adult older kid one reason good talker age good boy said bare acknowledg babi blue remain focus televis movi almost megan knew better slip bedroom finish get readi time look mason face grate look noth like father platinum blond hair blue eye complet build take father megan diminut 5 3 davi 6 1 two hundr pound alreadi regist chart height weight accord pediatrician seen mason twice day born day came home hospit interest pictur email megan sent profession footbal career rise davi want shackl respons babi want spend time field parti hour night paid child support megan threaten wage garnish dread day mason old enough ask father never want anyth world hurt knew reject father would sigh step dress slid hip around get zipper way caus huff puff back mirror turn fro take appear alway love dress made feel sexi time respect boast sweetheart necklin hemlin fell knee put high school graduat gift uncl aidan ankl often call mother babi brother son famili born eight half\n",
      "Preprocessed Text: usual would tear around live room play toy one look minion sent practic cataton megan plan got dress earlier seen movi almost mistak consid littl young pg cartoon older cousin along brother mason often expo thing older like think surround adult older kid one reason good talker age good boy said bare acknowledg babi blue remain focu televi movi almost megan knew better slip bedroom finish get readi time look mason face grate look noth like father platinum blond hair blue eye complet build take father megan diminut 5 3 davi 6 1 two hundr pound alreadi regist chart height weight accord pediatrician seen mason twice day born day came home hospit interest pictur email megan sent profess footbal career rise davi want shackl respon babi want spend time field parti hour night paid child support megan threaten wage garnish dread day mason old enough ask father never want anyth world hurt knew reject father would sigh step dress slid hip around get zipper way cau huff puff back mirror turn fro take appear alway love dress made feel sexi time respect boast sweetheart necklin hemlin fell knee put high school graduat gift uncl aidan ankl often call mother babi brother son famili born eight half\n",
      "\n",
      "Original Text: first grandchild megan spent lot time grandpar turn meant spent lot time aidan devot hour hold spoil rotten came time talk could seem get uncl aidan call ankl nicknam stuck even marri question want godfath mason extrem honor wife emma ask son noah godmoth love newest cousin much plan best godmoth could step bedroom found mason yet move okay buddi time go start whine shook head fun day ahead us noah baptism parti uncl aidan aunt emma hous beau ask laugh ye get see play beau went couch pick could help find amus everyon go see today excit aidan emma black lab beau day place would get dog love much deni oomph mutter start basement stair heawi ask ye get big heavi boy made kitchen megan paus catch breath second mother breez sean youngest brother gavin readi ask nod like teenag file behind parent head garag want drive gavin said\n",
      "Preprocessed Text: first grandchild megan spent lot time grandpar turn meant spent lot time aidan devot hour hold spoil rotten came time talk could seem get uncl aidan call ankl nicknam stuck even marri question want godfath mason extrem honor wife emma ask son noah godmoth love newest cousin much plan best godmoth could step bedroom found mason yet move okay buddi time go start whine shook head fun day ahead us noah baptism parti uncl aidan aunt emma hou beau ask laugh ye get see play beau went couch pick could help find amu everyon go see today excit aidan emma black lab beau day place would get dog love much deni oomph mutter start basement stair heawi ask ye get big heavi boy made kitchen megan pau catch breath second mother breez sean youngest brother gavin readi ask nod like teenag file behind parent head garag want drive gavin said\n",
      "\n",
      "Original Text: smirk sean repli like gon na let drive car slid driver seat gavin reluctantli walk around passeng side see mother call acknowledg two finger salut crank pull driveway work get mason car seat parent land rover safe strap buckl hop besid parent rattl along made way suburb megan grown might look mark charact unw mother live rel life though cheerlead ran popular crowd school rare parti excess focus get good grade time heart set go medic school becom doctor time littl girl want noth help peopl alway mend bird broken wing tri resuscit squirrel hit car ditch play princess play hospit desir becom doctor need best score best activ gener shun temptat lead right path even manag bypass usual freshman crazi went univers georgia fell love first time life threw everyth away could say first love davi mason father anoth footbal player time run back uga captur later broke heart year later ran fast crowd parti drank much control possess want time littl time studi grade alreadi toilet unprepar emot breakdown experienc carsyn broke stop go class end flunk semest time got back track grade abandon hope medic school decid would becom nurs would fulfil need care sick peopl cours relationship davi end derail shortli graduat got pregnant unexpectedli take sever semest mason born year origin plan graduat excit everyth happen final finish mother voic brought megan thought\n",
      "Preprocessed Text: smirk sean repli like gon na let drive car slid driver seat gavin reluctantli walk around passeng side see mother call acknowledg two finger salut crank pull driveway work get mason car seat parent land rover safe strap buckl hop besid parent rattl along made way suburb megan grown might look mark charact unw mother live rel life though cheerlead ran popular crowd school rare parti excess focu get good grade time heart set go medic school becom doctor time littl girl want noth help peopl alway mend bird broken wing tri resuscit squirrel hit car ditch play princess play hospit desir becom doctor need best score best activ gener shun temptat lead right path even manag bypass usual freshman crazi went univ georgia fell love first time life threw everyth away could say first love davi mason father anoth footbal player time run back uga captur later broke heart year later ran fast crowd parti drank much control possess want time littl time studi grade alreadi toilet unprepar emot breakdown experienc carsyn broke stop go class end flunk semest time got back track grade abandon hope medic school decid would becom nur would fulfil need care sick peopl cour relationship davi end derail shortli graduat got pregnant unexpectedli take sever semest mason born year origin plan graduat excit everyth happen final finish mother voic brought megan thought\n",
      "\n",
      "Original Text: said pleasantli forward seat megan eye clock dashboard surpris see arriv half hour baptism start thing mother pride time lend hand start church mother reach mason take go see emma need help megan bent kiss mason cheek see littl sweeti grin happili dodg mother arm father instead made megan smile man man alreadi love sit brother couch watch tv good mani male role model hope inherit much father person megan watch disappear crowd famili friend wait church alcov bypass everyon turn right head hallway last door right knock megan emma best friend casey answer door well fairi godmoth muse grin megan step insid casey threw arm around met time hard like emma vivaci outgo friend long brown hair pull back lose knot wore demur black slip dress heel go megan ask gaze noah diaper na form emma feed bottl upper bodi drape towel cover saw emma wear signatur color green noah suck bottl twirl strand emma auburn hair finger father son fan emma wear hair grin good guess mean lot experi baptism megan laugh motion towel noah\n",
      "Preprocessed Text: said pleasantli forward seat megan eye clock dashboard surpri see arriv half hour baptism start thing mother pride time lend hand start church mother reach mason take go see emma need help megan bent kiss mason cheek see littl sweeti grin happili dodg mother arm father instead made megan smile man man alreadi love sit brother couch watch tv good mani male role model hope inherit much father person megan watch disappear crowd famili friend wait church alcov bypass everyon turn right head hallway last door right knock megan emma best friend casey answer door well fairi godmoth muse grin megan step insid casey threw arm around met time hard like emma vivaci outgo friend long brown hair pull back lose knot wore demur black slip dress heel go megan ask gaze noah diaper na form emma feed bottl upper bodi drape towel cover saw emma wear signatur color green noah suck bottl twirl strand emma auburn hair finger father son fan emma wear hair grin good guess mean lot experi baptism megan laugh motion towel noah\n",
      "\n",
      "Original Text: look like take proper like dress nod emma repli tell sinc gown old megan eye laci baptism gown hang closet door recogn pictur ankl baptism worn pass son snort sure aidan would appreci allud fact gown antiqu thu turn say old emma laugh sure would cours probabl argu gown might held still look fabul much younger age megan smile sound like bent emma rub one noah hand grab thumb fist held dear life aw love godmoth noah emma ask momentarili stop suck bottl flash quick smile warm megan heart sweet boy muse charmer like old man casey muse megan agre posit cock head casey sure fine godmoth casey wave hand dismiss honey last thing need respons plan spoil noah rotten corrupt good aunti emma roll eye satisfi choic megan worri godfath\n",
      "Preprocessed Text: look like take proper like dress nod emma repli tell sinc gown old megan eye laci baptism gown hang closet door recogn pictur ankl baptism worn pass son snort sure aidan would appreci allud fact gown antiqu thu turn say old emma laugh sure would cour probabl argu gown might held still look fabul much younger age megan smile sound like bent emma rub one noah hand grab thumb fist held dear life aw love godmoth noah emma ask momentarili stop suck bottl flash quick smile warm megan heart sweet boy muse charmer like old man casey muse megan agr posit cock head casey sure fine godmoth casey wave hand dismiss honey last thing need respon plan spoil noah rotten corrupt good aunti emma roll eye satisfi choic megan worri godfath\n",
      "\n",
      "Original Text: know part famili casey gasp hand flew chest dramat mean never met mcdreami bollywood megan shrug mean heard flew aidan home time noah birth notic plead look emma exchang casey know casey tap chin index finger hmm know good godfath wink megan first serious delect mean man like sex stick hair dark eye built like brick shithous megan suddenli felt interest piqu imagin godfath would long time sinc date anyon long time sinc sex anyon spent last two year complet dateless sinc davi broke could practic join one local parish nun long abstain realli mmm hmm remind bollywood actor john abraham casey said snort sinc watch bollywood movi sinc one nate friend ask us indian film festiv casey grin megan besid fact seriou looker also kind compassion overal wonder man realli\n",
      "Preprocessed Text: know part famili casey gasp hand flew chest dramat mean never met mcdreami bollywood megan shrug mean heard flew aidan home time noah birth notic plead look emma exchang casey know casey tap chin index finger hmm know good godfath wink megan first seriou delect mean man like sex stick hair dark eye built like brick shithou megan suddenli felt interest piqu imagin godfath would long time sinc date anyon long time sinc sex anyon spent last two year complet dateless sinc davi broke could practic join one local parish nun long abstain realli mmm hmm remind bollywood actor john abraham casey said snort sinc watch bollywood movi sinc one nate friend ask us indian film festiv casey grin megan besid fact seriou looker also kind compass over wonder man realli\n",
      "\n",
      "Original Text: megan question load doctor man sound better better minut singl emma made strangl nois casey repli oh yeah singl widow actual megan purs lip prospect usual fell two still devast wive death readi fun live littl certainli hope pesh guy fell second categori anyth want littl fun realli think might interest date pesh emma ask move noah shoulder burp shrug megan repli date fun like could use emma grimac wipe noah face exactli told aidan tri fix two mean pesh need relationship wife death easi date front happen casey snort emma shot death glare noth need get involv someon interest relationship em might well tell casey urg glanc two tell fine emma huf resignedli hand noah casey make use chang diaper casey got busi clean noah emma turn megan\n",
      "Preprocessed Text: megan question load doctor man sound better better minut singl emma made strangl noi casey repli oh yeah singl widow actual megan pur lip prospect usual fell two still devast wive death readi fun live littl certainli hope pesh guy fell second categori anyth want littl fun realli think might interest date pesh emma ask move noah shoulder burp shrug megan repli date fun like could use emma grimac wipe noah face exactli told aidan tri fix two mean pesh need relationship wife death easi date front happen casey snort emma shot death glare noth need get involv someon interest relationship em might well tell casey urg glanc two tell fine emma huf resignedli hand noah casey make use chang diaper casey got busi clean noah emma turn megan\n",
      "\n",
      "Original Text: aidan broke meant say left aidan show noah gender ultrasound cheat casey interrupt wave around wet wipe hand close eye moment shake head ye right much bring aspect today day casey grip noah ankl hoist littl butt slide new diaper welcom anyway say megan press right aidan broken met pesh papa fitzgerald day heart attack vfw pesh doctor treat struck friendship megan ask grimac exactli see patrick want forc aidan realli fight felt best way would competit megan felt eye widen surpris papa want pesh date yeah emma momentarili distract casey blow raspberri noah stomach smile spread across face noah kick leg giggl casey want get dress casey ask glanc mind bring hand hip megan huf exasper breath um could pleas focu minut consid bomb drop sorri want tell mean whatev us past\n",
      "Preprocessed Text: aidan broke meant say left aidan show noah gender ultrasound cheat casey interrupt wave around wet wipe hand close eye moment shake head ye right much bring aspect today day casey grip noah ankl hoist littl butt slide new diaper welcom anyway say megan press right aidan broken met pesh papa fitzgerald day heart attack vfw pesh doctor treat struck friendship megan ask grimac exactli see patrick want forc aidan realli fight felt best way would competit megan felt eye widen surpri papa want pesh date yeah emma momentarili distract casey blow raspberri noah stomach smile spread across face noah kick leg giggl casey want get dress casey ask glanc mind bring hand hip megan huf exasp breath um could plea focu minut consid bomb drop sorri want tell mean whatev us past\n",
      "\n",
      "Original Text: neither one us realli care like thought megan said emma confus express wiggl eyebrow suggest face flush cours casey eye emma took baptism gown hanger make though one time certainli get second base emma argu wick grin casey ad piti could inform us go waist emma roll eye imposs snatch gown away casey start wrestl noah yard fabric like relationship realli even go togeth came hous bring dinner check bed rest took opera chemistri two megan ask respond emma pull noah sit posit start fasten row button back gown em megan press gave heavi sigh pick noah stare face respond great chemistri togeth amaz kisser got sens run right way physic part smart kind woman dream shook head matter aidan\n",
      "Preprocessed Text: neither one us realli care like thought megan said emma confu express wiggl eyebrow suggest face flush cour casey eye emma took baptism gown hanger make though one time certainli get second base emma argu wick grin casey ad piti could inform us go waist emma roll eye imposs snatch gown away casey start wrestl noah yard fabric like relationship realli even go togeth came hou bring dinner check bed rest took opera chemistri two megan ask respond emma pull noah sit posit start fasten row button back gown em megan press gave heavi sigh pick noah stare face respond great chemistri togeth amaz kisser got sen run right way physic part smart kind woman dream shook head matter aidan\n",
      "\n",
      "Original Text: could never ever love anoth man like give noah kiss cheek ad neither one us realli good tri make sure aidan realli one pressur famili friend tri date first time wife death megan cross arm chest get need hurt say fling go hurt mayb could give confid need go find real woman dream casey snort could man look like issu megan shrug never know marri long time wife die might find hard get back date world last woman care love someon els emma shook head trust fling kind guy want wife children although megan son quit readi marriag year right want date fun willingli sacrif much ensur mason could attent sinc short father greatest hurdl commit relationship want open son hurt might come get attach man date disappear broke ankl realli want fix us megan question nod regardless happen pesh broken aidan respect admir want pesh happi gave megan point look want megan wrinkl nose realli readi yet think long hard agre anyth pesh unknowingli persuas moment might forget resolv come back later hurt\n",
      "Preprocessed Text: could never ever love anoth man like give noah kiss cheek ad neither one us realli good tri make sure aidan realli one pressur famili friend tri date first time wife death megan cross arm chest get need hurt say fling go hurt mayb could give confid need go find real woman dream casey snort could man look like issu megan shrug never know marri long time wife die might find hard get back date world last woman care love someon el emma shook head trust fling kind guy want wife children although megan son quit readi marriag year right want date fun willingli sacrif much ensur mason could attent sinc short father greatest hurdl commit relationship want open son hurt might come get attach man date disappear broke ankl realli want fix us megan question nod regardless happen pesh broken aidan respect admir want pesh happi gave megan point look want megan wrinkl nose realli readi yet think long hard agr anyth pesh unknowingli persua moment might forget resolv come back later hurt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for index, row in df.head(10).iterrows():\n",
    "    print(f\"Original Text: {row['Text']}\")\n",
    "    print(f\"Preprocessed Text: {preprocess_text(row['Text'])}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    Text  Label\n",
      "26541  procedur perform dddr perman insert screw righ...    6.0\n",
      "8588   respond 1 13 assist engin state madhya pradesh...    5.0\n",
      "54173  struction detect input word match mask corresp...    7.0\n",
      "55043  frontier commun corpor satellit oper dish netw...    2.0\n",
      "53437  rnce signal speed ref speed limit speed ref sp...    7.0\n",
      "...                                                  ...    ...\n",
      "3767   sleep well ever sinc arriv feel new take grant...    3.0\n",
      "1118   servant happi wo sleep zayn neither sabotag co...    3.0\n",
      "21546  kerala forest act 1961 regul preserv forest fo...    5.0\n",
      "16809  hindu execut direct wife sell properti utilis ...    5.0\n",
      "7950   respond 1 obtain mortgag decre rs one rao raja...    5.0\n",
      "\n",
      "[5000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df_shuffled = df.sample(frac=1, random_state=42)\n",
    "subset_df = df_shuffled.head(5000)\n",
    "\n",
    "print(subset_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df.to_csv('merged_data_subset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Distribution:\n",
      "Label\n",
      "5.0    1309\n",
      "1.0    1096\n",
      "4.0     983\n",
      "3.0     483\n",
      "6.0     390\n",
      "2.0     348\n",
      "7.0     250\n",
      "8.0     141\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "label_distribution = subset_df['Label'].value_counts()\n",
    "\n",
    "print(\"Label Distribution:\")\n",
    "print(label_distribution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Label'] = df['Label'] - 1\n",
    "train_df, val_df = train_test_split(subset_df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kkysh\\AppData\\Local\\Temp\\ipykernel_19452\\3778342067.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(train_encodings['input_ids']),\n",
      "C:\\Users\\kkysh\\AppData\\Local\\Temp\\ipykernel_19452\\3778342067.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(train_encodings['attention_mask']),\n"
     ]
    }
   ],
   "source": [
    "train_encodings = tokenizer(list(train_df['Text']), truncation=True, padding=True, max_length=256, return_tensors='pt')\n",
    "train_dataset = TensorDataset(\n",
    "    torch.tensor(train_encodings['input_ids']),\n",
    "    torch.tensor(train_encodings['attention_mask']),\n",
    "    torch.tensor(train_df['Label'].values, dtype=torch.long)  # Assuming labels are integers\n",
    ")\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kkysh\\AppData\\Local\\Temp\\ipykernel_19452\\2603499773.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(val_encodings['input_ids']),\n",
      "C:\\Users\\kkysh\\AppData\\Local\\Temp\\ipykernel_19452\\2603499773.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(val_encodings['attention_mask']),\n"
     ]
    }
   ],
   "source": [
    "val_encodings = tokenizer(list(val_df['Text']), truncation=True, padding=True, max_length=256, return_tensors='pt')\n",
    "val_dataset = TensorDataset(\n",
    "    torch.tensor(val_encodings['input_ids']),\n",
    "    torch.tensor(val_encodings['attention_mask']),\n",
    "    torch.tensor(val_df['Label'].values, dtype=torch.long)  # Assuming labels are integers\n",
    ")\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages\\transformers\\optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=8, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 3  # Adjust as needed\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 7. 4. 5. 0. 3. 6. 1.]\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training:   0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training:   3%|▎         | 14/500 [06:26<3:43:39, 27.61s/it]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Target 8 is out of bounds.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m input_ids, attention_mask, labels \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mto(device), attention_mask\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      7\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m----> 8\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m     10\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:1232\u001b[0m, in \u001b[0;36mRobertaForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1230\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mproblem_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msingle_label_classification\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1231\u001b[0m     loss_fct \u001b[38;5;241m=\u001b[39m CrossEntropyLoss()\n\u001b[1;32m-> 1232\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1233\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mproblem_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulti_label_classification\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1234\u001b[0m     loss_fct \u001b[38;5;241m=\u001b[39m BCEWithLogitsLoss()\n",
      "File \u001b[1;32mc:\\Users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1180\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1181\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kkysh\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\functional.py:3059\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3058\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3059\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mIndexError\u001b[0m: Target 8 is out of bounds."
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1} Training\"):\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=f\"Epoch {epoch + 1} Validation\"):\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    report = classification_report(all_labels, all_preds, target_names=list(categories.values()))\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}, Validation Accuracy: {accuracy:.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "0.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
